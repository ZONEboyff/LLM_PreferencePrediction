{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T07:53:11.366640Z",
     "start_time": "2025-01-26T07:53:10.896982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/submission.csv')\n",
    "submission.head()"
   ],
   "id": "6765f610e3ae6d07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           id  winner_model_a  winner_model_b  winner_tie\n",
       "0  2979577167        0.254899        0.371648    0.373453\n",
       "1  2979584770        0.315672        0.306331    0.377997\n",
       "2  2979732971        0.264466        0.567645    0.167889\n",
       "3  2979751183        0.500031        0.217677    0.282292\n",
       "4  2979854448        0.335786        0.301980    0.362234"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2979577167</td>\n",
       "      <td>0.254899</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.373453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2979584770</td>\n",
       "      <td>0.315672</td>\n",
       "      <td>0.306331</td>\n",
       "      <td>0.377997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2979732971</td>\n",
       "      <td>0.264466</td>\n",
       "      <td>0.567645</td>\n",
       "      <td>0.167889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2979751183</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.217677</td>\n",
       "      <td>0.282292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2979854448</td>\n",
       "      <td>0.335786</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.362234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T09:49:49.377784Z",
     "start_time": "2025-01-25T09:42:31.910632Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class LLMPreferenceAnalyzer:\n",
    "    def __init__(self, train_path='data/train.csv', test_path='data/test.csv'):\n",
    "        self.train_df = pd.read_csv(train_path)\n",
    "        self.test_df = pd.read_csv(test_path)\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Basic preprocessing\n",
    "        def compute_text_features(row, model):\n",
    "            response = row[f'response_{model}']\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            # Length features\n",
    "            features = {\n",
    "                f'{model}_response_length': len(str(response)),\n",
    "                f'{model}_word_count': len(str(response).split()),\n",
    "                \n",
    "                # Sentiment analysis\n",
    "                f'{model}_sentiment_compound': self.sia.polarity_scores(str(response))['compound'],\n",
    "                f'{model}_sentiment_pos': self.sia.polarity_scores(str(response))['pos'],\n",
    "                f'{model}_sentiment_neg': self.sia.polarity_scores(str(response))['neg'],\n",
    "                f'{model}_sentiment_neu': self.sia.polarity_scores(str(response))['neu'],\n",
    "            }\n",
    "            return pd.Series(features)\n",
    "        \n",
    "        # Add text features for both models\n",
    "        self.train_df = pd.concat([\n",
    "            self.train_df, \n",
    "            self.train_df.apply(lambda row: compute_text_features(row, 'a'), axis=1),\n",
    "            self.train_df.apply(lambda row: compute_text_features(row, 'b'), axis=1)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Encode target variable\n",
    "        self.train_df['target'] = np.nan\n",
    "        self.train_df.loc[self.train_df['winner_model_a'] == 1, 'target'] = 0\n",
    "        self.train_df.loc[self.train_df['winner_model_b'] == 1, 'target'] = 1\n",
    "        self.train_df.loc[self.train_df['winner_tie'] == 1, 'target'] = 2\n",
    "        \n",
    "        return self.train_df\n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        # Prepare features and target\n",
    "        features = [col for col in self.train_df.columns if col not in ['id', 'prompt', 'response_a', 'response_b', 'model_a', 'model_b', \n",
    "                                                                        'winner_model_a', 'winner_model_b', 'winner_tie', 'target']]\n",
    "        X = self.train_df[features]\n",
    "        y = self.train_df['target']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def explore_data(self):\n",
    "        # Basic data exploration\n",
    "        print(\"Training Data Overview:\")\n",
    "        print(self.train_df['target'].value_counts(normalize=True))\n",
    "        \n",
    "        # Correlation analysis\n",
    "        print(\"\\nFeature Correlations with Target:\")\n",
    "        correlation_features = [col for col in self.train_df.columns if col.startswith(('a_', 'b_'))]\n",
    "        print(self.train_df[correlation_features + ['target']].corr()['target'].sort_values(ascending=False))\n",
    "\n",
    "# Initialize and run analysis\n",
    "analyzer = LLMPreferenceAnalyzer()\n",
    "processed_df = analyzer.preprocess_data()\n",
    "X_train, X_val, y_train, y_val = analyzer.split_data()\n",
    "analyzer.explore_data()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Overview:\n",
      "target\n",
      "0.0    0.349225\n",
      "1.0    0.343775\n",
      "2.0    0.307000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature Correlations with Target:\n",
      "target                  1.000000\n",
      "b_sentiment_neu         0.048594\n",
      "a_sentiment_neu         0.037009\n",
      "a_sentiment_neg        -0.005198\n",
      "b_word_count           -0.008952\n",
      "b_response_length      -0.009091\n",
      "b_sentiment_neg        -0.022440\n",
      "b_sentiment_compound   -0.027534\n",
      "a_sentiment_pos        -0.041481\n",
      "b_sentiment_pos        -0.043757\n",
      "a_sentiment_compound   -0.046784\n",
      "a_word_count           -0.082780\n",
      "a_response_length      -0.083468\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T09:41:43.242213800Z",
     "start_time": "2025-01-25T09:34:59.371200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.head()\n",
    "df.columns()"
   ],
   "id": "b1b647547e2d510e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T14:39:35.991071Z",
     "start_time": "2025-01-25T14:36:54.292368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class LLMPreferenceModel:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = AutoModel.from_pretrained('bert-base-uncased').to(self.device)\n",
    "        \n",
    "    def extract_bert_embeddings(self, texts):\n",
    "        # Encode texts with BERT\n",
    "        inputs = self.tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    def feature_engineering(self, df):\n",
    "        # Text length features\n",
    "        df['prompt_length'] = df['prompt'].str.len()\n",
    "        df['response_a_length'] = df['response_a'].str.len()\n",
    "        df['response_b_length'] = df['response_b'].str.len()\n",
    "        \n",
    "        # BERT embeddings for prompt and responses\n",
    "        df['prompt_embedding'] = list(self.extract_bert_embeddings(df['prompt']))\n",
    "        df['response_a_embedding'] = list(self.extract_bert_embeddings(df['response_a']))\n",
    "        df['response_b_embedding'] = list(self.extract_bert_embeddings(df['response_b']))\n",
    "        \n",
    "        # TF-IDF features\n",
    "        tfidf = TfidfVectorizer(max_features=100)\n",
    "        prompt_tfidf = tfidf.fit_transform(df['prompt']).toarray()\n",
    "        \n",
    "        # Combine features\n",
    "        embedding_features = np.hstack([\n",
    "            df['prompt_embedding'].tolist(),\n",
    "            df['response_a_embedding'].tolist(),\n",
    "            df['response_b_embedding'].tolist()\n",
    "        ])\n",
    "        \n",
    "        # Combine all features\n",
    "        X = np.hstack([\n",
    "            embedding_features,\n",
    "            prompt_tfidf,\n",
    "            df[['prompt_length', 'response_a_length', 'response_b_length']]\n",
    "        ])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def prepare_target(self, df):\n",
    "        # Encode target\n",
    "        target = np.zeros(len(df))\n",
    "        target[df['winner_model_a'] == 1] = 0\n",
    "        target[df['winner_model_b'] == 1] = 1\n",
    "        target[df['winner_tie'] == 1] = 2\n",
    "        return target\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val):\n",
    "        # LightGBM parameters\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 3,\n",
    "            'metric': 'multi_logloss',\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5\n",
    "        }\n",
    "        \n",
    "        # Create datasets\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params, \n",
    "            train_data, \n",
    "            valid_sets=[train_data, val_data],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict_probabilities(self, model, X_test):\n",
    "        # Predict probabilities\n",
    "        probs = model.predict(X_test)\n",
    "        return probs\n",
    "    \n",
    "    def prepare_submission(self, test_df, probs):\n",
    "        # Create submission DataFrame\n",
    "        submission = pd.DataFrame({\n",
    "            'id': test_df['id'],\n",
    "            'winner_model_a': probs[:, 0],\n",
    "            'winner_model_b': probs[:, 1],\n",
    "            'winner_tie': probs[:, 2]\n",
    "        })\n",
    "        return submission\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load data\n",
    "    train_df = pd.read_csv('data/train.csv')\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "    \n",
    "    # Initialize model\n",
    "    llm_model = LLMPreferenceModel()\n",
    "    \n",
    "    # Prepare training data\n",
    "    X = llm_model.feature_engineering(train_df)\n",
    "    y = llm_model.prepare_target(train_df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train model\n",
    "    model = llm_model.train_model(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = llm_model.feature_engineering(test_df)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    test_probs = llm_model.predict_probabilities(model, X_test)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = llm_model.prepare_submission(test_df, test_probs)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "2652027f2e9182",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cad23cb8b99544db95dd84adf84b8ed8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prati\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6f84faf2d3748e69289bc93b09d41b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2442\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2441\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2442\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[0;32m   2443\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:114\u001B[0m, in \u001B[0;36mBertTokenizer.__init__\u001B[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    100\u001B[0m     vocab_file,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    113\u001B[0m ):\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(vocab_file):\n\u001B[0;32m    115\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    116\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a vocabulary file at path \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvocab_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. To load the vocabulary from a Google pretrained\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    117\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    118\u001B[0m         )\n",
      "\u001B[1;31mTypeError\u001B[0m: isfile: path should be string, bytes, os.PathLike or integer, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 138\u001B[0m\n\u001B[0;32m    135\u001B[0m     submission\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubmission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 138\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[2], line 113\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    110\u001B[0m test_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# Initialize model\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m llm_model \u001B[38;5;241m=\u001B[39m LLMPreferenceModel()\n\u001B[0;32m    115\u001B[0m \u001B[38;5;66;03m# Prepare training data\u001B[39;00m\n\u001B[0;32m    116\u001B[0m X \u001B[38;5;241m=\u001B[39m llm_model\u001B[38;5;241m.\u001B[39mfeature_engineering(train_df)\n",
      "Cell \u001B[1;32mIn[2], line 14\u001B[0m, in \u001B[0;36mLLMPreferenceModel.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert_model \u001B[38;5;241m=\u001B[39m AutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:926\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    923\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 926\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class_fast\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    928\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2208\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2205\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2206\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2208\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[0;32m   2209\u001B[0m     resolved_vocab_files,\n\u001B[0;32m   2210\u001B[0m     pretrained_model_name_or_path,\n\u001B[0;32m   2211\u001B[0m     init_configuration,\n\u001B[0;32m   2212\u001B[0m     \u001B[38;5;241m*\u001B[39minit_inputs,\n\u001B[0;32m   2213\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2214\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2215\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m   2216\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[0;32m   2217\u001B[0m     _is_local\u001B[38;5;241m=\u001B[39mis_local,\n\u001B[0;32m   2218\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2219\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2220\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2246\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2243\u001B[0m \u001B[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001B[39;00m\n\u001B[0;32m   2244\u001B[0m \u001B[38;5;66;03m# loaded directly from the GGUF file.\u001B[39;00m\n\u001B[0;32m   2245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (from_slow \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_tokenizer_file) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gguf_file:\n\u001B[1;32m-> 2246\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class)\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[0;32m   2247\u001B[0m         copy\u001B[38;5;241m.\u001B[39mdeepcopy(resolved_vocab_files),\n\u001B[0;32m   2248\u001B[0m         pretrained_model_name_or_path,\n\u001B[0;32m   2249\u001B[0m         copy\u001B[38;5;241m.\u001B[39mdeepcopy(init_configuration),\n\u001B[0;32m   2250\u001B[0m         \u001B[38;5;241m*\u001B[39minit_inputs,\n\u001B[0;32m   2251\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2252\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2253\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m   2254\u001B[0m         _commit_hash\u001B[38;5;241m=\u001B[39m_commit_hash,\n\u001B[0;32m   2255\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)),\n\u001B[0;32m   2256\u001B[0m     )\n\u001B[0;32m   2257\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2258\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2443\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2441\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2442\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[1;32m-> 2443\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n\u001B[0;32m   2444\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2445\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2446\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2447\u001B[0m     )\n\u001B[0;32m   2448\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:87\u001B[0m, in \u001B[0;36mimport_protobuf_decode_error\u001B[1;34m(error_message)\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DecodeError\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PROTOBUF_IMPORT_ERROR\u001B[38;5;241m.\u001B[39mformat(error_message))\n",
      "\u001B[1;31mImportError\u001B[0m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import google.protobuf\n",
    "print(google.protobuf.__version__)\n"
   ],
   "id": "fd7129ae3ff832d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
