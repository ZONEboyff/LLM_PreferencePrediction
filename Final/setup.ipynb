{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **LLM Model Preference Prediction**\n",
    "## **Competition Notebook**\n",
    "\n",
    "### **Overview**\n",
    "This notebook is designed to address the **LLM Model Preference Prediction** challenge. The goal is to predict the preferred LLM response (or a tie) for given prompts, based on their quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Objectives**\n",
    "1. **Feature Engineering**:\n",
    "   - Extract embeddings using a pretrained RoBERTa model.\n",
    "   - Engineer additional features such as text lengths to enhance prediction performance.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - Use a LightGBM model with a multiclass objective to classify preferences.\n",
    "   - Train on either a validation split or the full training dataset, depending on the evaluation setup.\n",
    "\n",
    "3. **Submission Preparation**:\n",
    "   - Generate predictions for the test set and create a `submission.csv` file in the required format.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ],
   "id": "701eb1ada1ec9e5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup and Environment**\n",
    "### **Purpose**\n",
    "This section ensures that the required libraries and dependencies are installed and configured correctly. It also verifies the system's readiness to handle computations, including GPU availability for accelerating operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps**\n",
    "1. **Install Dependencies**:\n",
    "   - Use popular libraries like NumPy, Pandas, LightGBM, Transformers, and Scikit-learn.\n",
    "2. **Set up Device**:\n",
    "   - Check for GPU availability to optimize embedding extraction using RoBERTa.\n",
    "3. **Random Seed Initialization**:\n",
    "   - Fix random seeds for reproducibility across all operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Matters**\n",
    "- **Smooth Execution**: Ensures the code runs seamlessly without missing dependencies.\n",
    "- **Performance Optimization**: Utilizes GPU for faster embedding generation.\n",
    "- **Reproducibility**: Consistent results every time the notebook is executed.\n",
    "\n"
   ],
   "id": "f7713d4a99c8e0fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing with RoBERTa Embeddings and Feature Engineering\n",
    "\n",
    "This section focuses on preprocessing textual data for a machine learning classification task. The preprocessing steps are designed to leverage the power of HuggingFace's **RoBERTa** model for generating text embeddings and include additional feature engineering for better model performance. \n",
    "\n",
    "### Key Steps:\n",
    "1. **Extracting Text Embeddings**: \n",
    "   - Utilizes the `roberta-base` model to generate embeddings for textual inputs (`prompt`, `response_a`, `response_b`). \n",
    "   - Embeddings are based on the `[CLS]` token from the last hidden layer of the model.\n",
    "   \n",
    "2. **Feature Engineering**: \n",
    "   - Adds numeric features such as the lengths of the `prompt` and responses (`response_a`, `response_b`) to enrich the feature space.\n",
    "\n",
    "3. **Preparing Target Labels**:\n",
    "   - Converts multi-label information from the columns `winner_model_a`, `winner_model_b`, and `winner_tie` into integer-based classes:\n",
    "     - `0`: Model A is preferred.\n",
    "     - `1`: Model B is preferred.\n",
    "     - `2`: Tie.\n",
    "\n",
    "4. **Saving Processed Data**: \n",
    "   - Saves the engineered features and target labels as `.npy` files for efficient storage and model training.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Requirements:\n",
    "- **Train Dataset (`train.csv`)**:\n",
    "  - Columns:\n",
    "    - `prompt`: The textual input or query.\n",
    "    - `response_a`: Response generated by Model A.\n",
    "    - `response_b`: Response generated by Model B.\n",
    "    - `winner_model_a`: Binary indicator for Model A being the preferred choice (1 if true, 0 otherwise).\n",
    "    - `winner_model_b`: Binary indicator for Model B being the preferred choice (1 if true, 0 otherwise).\n",
    "    - `winner_tie`: Binary indicator for a tie between Model A and Model B (1 if true, 0 otherwise).\n",
    "    \n",
    "- **Test Dataset (`test.csv`)**:\n",
    "  - Columns:\n",
    "    - `prompt`: The textual input or query.\n",
    "    - `response_a`: Response generated by Model A.\n",
    "    - `response_b`: Response generated by Model B.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs:\n",
    "- **Processed Features**:\n",
    "  - `x_train.npy`: A NumPy array containing the combined features (embeddings + numeric features) for training data.\n",
    "  - `x_test.npy`: A NumPy array containing the combined features for test data.\n",
    "  \n",
    "- **Target Labels**:\n",
    "  - `y_train.npy`: A NumPy array containing the target labels for training data.\n",
    "\n",
    "---\n",
    "\n",
    "### Highlights:\n",
    "- **Batch Processing**: Efficiently processes text data in batches to minimize memory usage.\n",
    "- **GPU Acceleration**: Automatically leverages GPU if available for faster embedding generation.\n",
    "- **Reproducibility**: Ensures consistent results by setting a fixed random seed.\n",
    "\n",
    "This preprocessing pipeline is designed to handle large datasets and creates feature-rich inputs for machine learning models, making it ideal for tasks like multi-class classification.\n"
   ],
   "id": "ce89c8cf904a0dba"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T17:09:49.654942Z",
     "start_time": "2025-01-28T17:09:46.671159Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing text data and extracting features for a machine learning model. \n",
    "    It includes methods to tokenize and extract embeddings from text using a pre-trained transformer model \n",
    "    (RoBERTa), as well as for feature engineering and preparing target labels.\n",
    "\n",
    "    Attributes:\n",
    "        device (torch.device): The device to run computations on ('cuda' if available, otherwise 'cpu').\n",
    "        tokenizer (AutoTokenizer): A tokenizer for tokenizing input text, initialized from 'roberta-base'.\n",
    "        embedding_model (AutoModel): A pre-trained transformer model for extracting text embeddings, initialized from 'roberta-base'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessor class by setting the computation device (CPU/GPU) and loading \n",
    "        the tokenizer and transformer model (RoBERTa).\n",
    "        \"\"\"\n",
    "        # Set the computation device to GPU if available, otherwise use CPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load RoBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        # Load RoBERTa model and move it to the computation device\n",
    "        self.embedding_model = AutoModel.from_pretrained('roberta-base').to(self.device)\n",
    "\n",
    "    def extract_embeddings(self, texts, batch_size=16):\n",
    "        \"\"\"\n",
    "        Extracts text embeddings using the RoBERTa model. The embeddings are taken from the [CLS] token \n",
    "        (first token of the last hidden state).\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series or list): A list or pandas Series of strings (text data).\n",
    "            batch_size (int): The batch size to use for processing text inputs.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array of shape (len(texts), embedding_size) containing the embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        all_embeddings = []  # List to store embeddings for all text batches\n",
    "\n",
    "        # Process texts in batches to handle memory efficiently\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            # Extract a batch of texts\n",
    "            batch = texts[i:i + batch_size].tolist()\n",
    "\n",
    "            # Tokenize the batch (convert text to token IDs) and move to the computation device\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512).to(self.device)\n",
    "\n",
    "            # Use the model to compute embeddings, disabling gradient computation for efficiency\n",
    "            with torch.no_grad():\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "\n",
    "            # Extract embeddings from the [CLS] token (first token) of the last hidden state\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "            # Append the batch embeddings to the list\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "        # Concatenate all batch embeddings into a single NumPy array\n",
    "        return np.concatenate(all_embeddings)\n",
    "\n",
    "    def feature_engineering(self, df):\n",
    "        \"\"\"\n",
    "        Performs feature engineering on the input DataFrame by extracting text embeddings and other numeric features.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): A pandas DataFrame containing 'prompt', 'response_a', and 'response_b' columns.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array of shape (len(df), feature_size) containing engineered features.\n",
    "        \"\"\"\n",
    "        # Add simple numeric features: the lengths of the prompt and responses\n",
    "        df['prompt_length'] = df['prompt'].str.len()\n",
    "        df['response_a_length'] = df['response_a'].str.len()\n",
    "        df['response_b_length'] = df['response_b'].str.len()\n",
    "\n",
    "        # Extract text embeddings for prompts and responses\n",
    "        df['prompt_embedding'] = list(self.extract_embeddings(df['prompt']))\n",
    "        df['response_a_embedding'] = list(self.extract_embeddings(df['response_a']))\n",
    "        df['response_b_embedding'] = list(self.extract_embeddings(df['response_b']))\n",
    "\n",
    "        # Combine embeddings into a single feature array\n",
    "        embeddings = np.hstack([\n",
    "            np.vstack(df['prompt_embedding']),       # Prompt embeddings\n",
    "            np.vstack(df['response_a_embedding']),   # Response A embeddings\n",
    "            np.vstack(df['response_b_embedding'])    # Response B embeddings\n",
    "        ])\n",
    "\n",
    "        # Combine embeddings with numeric features\n",
    "        numeric_features = df[['prompt_length', 'response_a_length', 'response_b_length']].values\n",
    "        X = np.hstack([embeddings, numeric_features])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def prepare_target(self, df):\n",
    "        \"\"\"\n",
    "        Prepares the target variable for classification. Converts the winner columns into a single integer label:\n",
    "        0 for 'winner_model_a', 1 for 'winner_model_b', and 2 for 'winner_tie'.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): A pandas DataFrame containing 'winner_model_a', 'winner_model_b', and 'winner_tie' columns.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A NumPy array of shape (len(df),) containing integer labels.\n",
    "        \"\"\"\n",
    "        # Initialize target array with zeros\n",
    "        target = np.zeros(len(df))\n",
    "\n",
    "        # Set target labels based on winner columns\n",
    "        target[df['winner_model_a'] == 1] = 0\n",
    "        target[df['winner_model_b'] == 1] = 1\n",
    "        target[df['winner_tie'] == 1] = 2\n",
    "\n",
    "        return target\n",
    "\n",
    "    def preprocess_and_save(self, train_path, test_path):\n",
    "        \"\"\"\n",
    "        Preprocesses training and test data by extracting features and targets, then saves them as NumPy arrays.\n",
    "\n",
    "        Args:\n",
    "            train_path (str): Path to the training CSV file.\n",
    "            test_path (str): Path to the test CSV file.\n",
    "        \"\"\"\n",
    "        # Load training and test datasets\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        # Preprocess and save training data\n",
    "        print(\"Processing training data...\")\n",
    "        X_train = self.feature_engineering(train_df)  # Extract features\n",
    "        np.save('x_train.npy', X_train)              # Save features as a NumPy array\n",
    "        y_train = self.prepare_target(train_df)      # Prepare target labels\n",
    "        np.save('y_train.npy', y_train)              # Save target labels as a NumPy array\n",
    "\n",
    "        # Preprocess and save test data\n",
    "        print(\"Processing test data...\")\n",
    "        X_test = self.feature_engineering(test_df)   # Extract features\n",
    "        np.save('x_test.npy', X_test)                # Save features as a NumPy array\n",
    "\n",
    "        print(\"Data preprocessing complete. NumPy arrays saved.\")\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## DatasetLLMPredictor: Training and Evaluating LLM Preference Prediction Model\n",
    "\n",
    "This script trains and evaluates a **LightGBM** model for the **LLM Model Preference Prediction** task. It is designed to handle multiclass classification, predicting the preference between two responses from different LLMs, or a tie.\n",
    "\n",
    "### Key Features and Workflow:\n",
    "1. **LightGBM Multiclass Classifier**:\n",
    "   - Objective: `multiclass` (for three classes: `winner_model_a`, `winner_model_b`, `winner_tie`).\n",
    "   - Automatically supports GPU acceleration if available, improving training efficiency.\n",
    "\n",
    "2. **Reproducibility**:\n",
    "   - Includes methods to save and reload the trained model for future use, enabling reproducible workflows.\n",
    "\n",
    "3. **Training and Evaluation**:\n",
    "   - Utilizes the entire training dataset without a validation split, focusing on learning from all available data.\n",
    "   - Provides:\n",
    "     - **Accuracy Score**: Overall correctness of predictions.\n",
    "     - **Classification Report**: Detailed metrics (precision, recall, F1-score) for each class.\n",
    "\n",
    "4. **Predictor Class Structure**:\n",
    "   - **Training**:\n",
    "     - Fits the model on the full training dataset and evaluates its performance.\n",
    "\n",
    "5. **Test Dataset Prediction**:\n",
    "   - The trained model can generate predictions for a separate test dataset, supporting model evaluation on unseen data.\n",
    "\n"
   ],
   "id": "a16777a4185431f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T18:13:46.546501Z",
     "start_time": "2025-01-28T18:13:46.538476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np  # For numerical operations and loading preprocessed data\n",
    "import pandas as pd  # For creating the submission DataFrame\n",
    "import lightgbm as lgb\n",
    "\n",
    "class DatasetLLMPredictor:\n",
    "    \"\"\"\n",
    "    A class to train and evaluate a LightGBM model on the entire dataset \n",
    "    for predicting the preference between LLM responses.\n",
    "\n",
    "    Attributes:\n",
    "        accuracy (float): The accuracy score of the model on the training dataset.\n",
    "        detailed_metrics (dict): Detailed classification metrics, including precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the predictor with attributes to store accuracy and detailed metrics.\n",
    "        \"\"\"\n",
    "        self.accuracy = None  # Placeholder for accuracy score\n",
    "        self.detailed_metrics = None  # Placeholder for detailed classification metrics\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Trains a LightGBM model on the provided training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (numpy.ndarray): Feature matrix for training.\n",
    "            y_train (numpy.ndarray): Labels corresponding to the training features.\n",
    "\n",
    "        Returns:\n",
    "            lgb.LGBMClassifier: Trained LightGBM model.\n",
    "        \"\"\"\n",
    "        # Initialize the LightGBM classifier with multiclass support\n",
    "        model = lgb.LGBMClassifier(\n",
    "            objective='multiclass',  # Specifies a multiclass classification objective\n",
    "            num_class=3  # Number of target classes (Model A, Model B, Tie)\n",
    "        )\n",
    "\n",
    "        # Train the model on the entire training dataset\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the training data for evaluation\n",
    "        train_pred = model.predict(X_train)\n",
    "\n",
    "        # Calculate accuracy on the training data\n",
    "        self.accuracy = accuracy_score(y_train, train_pred)\n",
    "\n",
    "        # Generate a detailed classification report\n",
    "        self.detailed_metrics = classification_report(y_train, train_pred, output_dict=True)\n",
    "\n",
    "        # Print results for transparency\n",
    "        print(f\"Training Accuracy: {self.accuracy:.4f}\")\n",
    "        print(\"Detailed Classification Report:\")\n",
    "        print(classification_report(y_train, train_pred))  # Human-readable report\n",
    "\n",
    "        return model  # Return the trained model\n",
    "\n",
    "    def save_model(self, model, filepath):\n",
    "        \"\"\"\n",
    "        Saves the trained LightGBM model to a file.\n",
    "\n",
    "        Args:\n",
    "            model (lgb.LGBMClassifier): The trained LightGBM model.\n",
    "            filepath (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        model.booster_.save_model(filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Loads a trained LightGBM model from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file.\n",
    "\n",
    "        Returns:\n",
    "            lgb.Booster: The loaded LightGBM model.\n",
    "        \"\"\"\n",
    "        model = lgb.Booster(model_file=filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model\n",
    "\n",
    "    def generate_submission_csv(self, model, X_test, filepath):\n",
    "        \"\"\"\n",
    "        Generates and saves a CSV submission file with class probabilities for the test dataset.\n",
    "    \n",
    "        Args:\n",
    "            model (lgb.Booster): The trained LightGBM model (Booster).\n",
    "            X_test (numpy.ndarray): Feature matrix for the test data.\n",
    "            test_ids (numpy.ndarray or pandas.Series): The actual IDs from the test dataset.\n",
    "            filepath (str): Path to save the submission CSV file.\n",
    "        \"\"\"\n",
    "        # Get the class probabilities for the test dataset using the Booster model's predict method\n",
    "        test_probs = model.predict(X_test, raw_score=False)  # Using raw_score=False to get probabilities\n",
    "        test_df = pd.read_csv('test.csv')  # Load the test dataset to get the IDs\n",
    "        # Create a DataFrame for the submission file\n",
    "        submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'winner_model_a': test_probs[:, 0],\n",
    "        'winner_model_b': test_probs[:, 1],\n",
    "        'winner_tie': test_probs[:, 2]})\n",
    "    \n",
    "        # Save the submission DataFrame as a CSV file\n",
    "        submission.to_csv(filepath, index=False)\n",
    "        print(f\"Submission file saved to {filepath}\")\n",
    "\n",
    "\n"
   ],
   "id": "42b38be65515bf20",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Usage: End-to-End Pipeline for Preprocessing, Training, and Prediction\n",
    "\n",
    "This section demonstrates the usage of the `DataPreprocessor` and `DatasetLLMPredictor` classes to preprocess the data, train the model, and generate predictions. The steps include:\n",
    "\n",
    "### Workflow:\n",
    "1. **Preprocessing**:\n",
    "   - Initialize the `DataPreprocessor` to preprocess both the training and test datasets.\n",
    "   - Save the processed features (`X_train`, `X_test`) and labels (`y_train`) as `.npy` files for reproducibility.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - Load the preprocessed features and labels.\n",
    "   - Use `DatasetLLMPredictor` to train a LightGBM model with the training data.\n",
    "   - Evaluate the model's performance on the training data.\n",
    "\n",
    "3. **Model Saving and Loading**:\n",
    "   - Save the trained LightGBM model to a file.\n",
    "   - Load the saved model for future use or inference.\n",
    "\n",
    "4. **Inference**:\n",
    "   - Generate predictions on the test dataset using the trained model.\n",
    "   - Save the predictions to a `.csv` file for submission.\n",
    "\n",
    "### Key Features:\n",
    "- Ensures reproducibility by saving intermediate outputs (`.npy` files and trained model).\n",
    "- Handles end-to-end machine learning workflow: data preprocessing, training, and prediction.\n",
    "\n"
   ],
   "id": "5b14b0f4dd0dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T18:15:31.909635Z",
     "start_time": "2025-01-28T18:14:58.267948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Initialize the Data Preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Step 2: Preprocess the data and save features and labels\n",
    "train_csv_path = \"train.csv\"  # Replace with the path to your train dataset\n",
    "test_csv_path = \"test.csv\"    # Replace with the path to your test dataset\n",
    "preprocessor.preprocess_and_save(train_csv_path, test_csv_path)\n",
    "\n",
    "# Step 3: Load the preprocessed features and labels\n",
    "X_train = np.load(\"x_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_test = np.load(\"x_test.npy\")\n",
    "\n",
    "# Step 4: Initialize the Dataset Predictor\n",
    "predictor = DatasetLLMPredictor()\n",
    "\n",
    "# Step 5: Train the model on the training data\n",
    "trained_model = predictor.train_model(X_train, y_train)\n",
    "\n",
    "# Step 6: Save the trained model for reproducibility\n",
    "predictor.save_model(trained_model, \"trained_lightgbm_model.txt\")\n",
    "\n",
    "# Step 7: Load the trained model (if needed)\n",
    "loaded_model = predictor.load_model(\"trained_lightgbm_model.txt\")\n",
    "\n",
    "# Generate and save the submission CSV\n",
    "predictor.generate_submission_csv(loaded_model, X_test, 'submission.csv')\n",
    "\n",
    "print(\"Pipeline executed successfully. Test predictions saved to 'test_predictions.csv'.\")\n"
   ],
   "id": "17b4e3c8b44822c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.678153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 588285\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 2307\n",
      "[LightGBM] [Info] Start training from score -1.052039\n",
      "[LightGBM] [Info] Start training from score -1.067768\n",
      "[LightGBM] [Info] Start training from score -1.180908\n",
      "Training Accuracy: 0.7702\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78     13969\n",
      "         1.0       0.76      0.80      0.78     13751\n",
      "         2.0       0.82      0.68      0.74     12280\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.78      0.77      0.77     40000\n",
      "weighted avg       0.77      0.77      0.77     40000\n",
      "\n",
      "Model saved to trained_lightgbm_model.txt\n",
      "Model loaded from trained_lightgbm_model.txt\n",
      "Submission file saved to submission.csv\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
